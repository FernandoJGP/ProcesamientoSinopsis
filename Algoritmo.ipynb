{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo del género de una película a raíz de su sinopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte I: Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se requiere la implementación de un algoritmo que, previamente entrenado, calcule el __género principal__ de una película en base al conocimiento adquirido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los géneros que tendrá en cuenta el algoritmo son: ___acción___, ___comedia___, ___terror___, ___bélico___ y __western__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los __archivos que contienen la sinopsis__ (o equivalente, pudiendo ser también un breve resumen de la primera parte de la película) de las películas estarán distribuídos de la siguiente forma: 1) Si forman parte del conjunto de pruebas estarán dentro de la carpeta del conjunto de prueba sin más, que será la carpeta en la que el algoritmo, una vez entrenado, buscará sinopsis para categorizarlas 2) Si forman parte del conjunto de entrenamiento estarán dentro de la carpeta del conjunto de entrenamiento y __a su vez__ dentro de una carpeta que indique su género."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente se planteó utilizar _html_ como __formato__ para almacenar los archivos con los que vamos a trabajar pero, dado que no ha sido posible encontrar una fuente común para extraer todas las sinopsis, finalmente se almacenarán como __texto plano__, trabajando de esta manera con archivos _.txt_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Los conocimientos requeridos por parte del usuario__ que ejecutará el algoritmo __son mínimos__: tan sólo necesita __colocar__ los textos, en el formato adecuado, en la carpeta indicada y __ejecutar__ el algoritmo en sí. Únicamente se requiere mayor interacción por parte del usuario si desea cambiar el funcionamiento del algorimo en sí, o ajustar el mismo, como por ejemplo si desea cambiar las palabras clave para cada una de las categorías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte II: Escaneo de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos símplemente indicando qué __carpeta__ contiene el __conjunto de entrenamiento__ y el de __prueba__, además de las __categorías__, que no serán útiles más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ruta_conjunto_entrenamiento = \"conjunto_entrenamiento\"\n",
    "ruta_conjunto_prueba = \"conjunto_prueba\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte II-A: Escaneo de las categorías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos escaneando la carpeta del conjunto de entrenamiento, que contendrá las __categorías__ en las que se podrán clasificar los nuevos documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'j', 'ch'}\n"
     ]
    }
   ],
   "source": [
    "import os # Nos ayudaremos de la librería \"os\" para leer ficheros y carpetas.\n",
    "\n",
    "categorías = {elemento for elemento in os.listdir(ruta_conjunto_entrenamiento) if os.path.isdir(ruta_conjunto_entrenamiento + \"/\" + elemento)} # \"os.listdir\" devuelve las el contenido de un directorio dado, pero además queremos filtrar que sea un directorio, por eso lo procesamos y le aplicamos el filtro de que sea un directorio.\n",
    "\n",
    "print(categorías)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente sección, vamos a recorrer nuestra estructura de carpetas para detectar cada uno de los textos a analizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte II-B: Escaneo del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a __encontrar__ todos los __archivos ya clasificados__ (conjunto de entrenamiento), según su __categoría__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a almacenar todos los archivos según su categoría en un diccionario, que contrendrá como clave la categoría y como valor un conjunto de archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, los valores (archivos) del diccionario estarán contenidos en conjuntos (sets) ya que no nos interesa el orden y, además, no permite duplicados (no puede haber dos archivos con el mismo nombre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archivos_entrenamiento = set()\n",
    "archivos_entrenamiento_categoría = {}\n",
    "\n",
    "for categoría in categorías:\n",
    "    conjunto_auxiliar = set()\n",
    "    \n",
    "    for fichero in os.listdir(ruta_conjunto_entrenamiento + \"/\" + categoría):\n",
    "        if fichero.endswith(\".txt\"):\n",
    "            nombre_y_ruta_fichero = categoría + \"/\" + fichero\n",
    "            conjunto_auxiliar.add(nombre_y_ruta_fichero)\n",
    "    \n",
    "    archivos_entrenamiento.update(conjunto_auxiliar)\n",
    "    archivos_entrenamiento_categoría[categoría] = conjunto_auxiliar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para __acceder a todos los archivos__, ejecutamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ch/2 Fast 2 Furious - A todo gas 2.txt',\n",
       " 'ch/A todo gas 3 punto 5 - Los bandoleros.txt',\n",
       " 'ch/A todo gas 4.txt',\n",
       " 'j/Apocalipsis now.txt'}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivos_entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para obtener el __número total de archivos__ dentro __del conjunto de entrenamiento__ simplemente ejecutamos la siguiente instrucción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(archivos_entrenamiento))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para __acceder a los archivos de una categoría__, por ejemplo acción, ejecutamos la siguiente orden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archivos_entrenamiento_categoría[\"acción\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener el __número de archivos dentro de una categoría__, ejecutamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(archivos_entrenamiento_categoría[\"acción\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte II-C: Escaneo del conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, a continuación, procedemos a __encontrar__ los __archivos que querríamos clasificar__ (conjunto de prueba)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez no es necesario usar diccionario (no necesitamos separarlos por categorías porque precisamente es lo que queremos hallar), así que usaremos un set por la misma razón por la que lo usamos como valor en el diccionario de los archivos del conjunto de entrenamiento (no queremos repeticiones ni nos interesa el orden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A todo gas - Tokyo race.txt'}\n"
     ]
    }
   ],
   "source": [
    "archivos_prueba = set()\n",
    "for file in os.listdir(ruta_conjunto_prueba):\n",
    "    if file.endswith(\".txt\"):\n",
    "        archivos_prueba.add(file)\n",
    "\n",
    "print(archivos_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte III: Elección del conjunto de palabras clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para ayudarnos en el estudio de las __palabras clave__ que debemos escoger para cada categoría vamos a realizar un pequeño estudio para determinar las palabras más frecuentes de cada categoría. La elección en sí debemos realizarla a mano puesto que no podemos escoger directamente las más frecuentes puesto que con total seguridad entre las más frecuentes se encontrarán verbos, conectores, preposiciones, pronombres, artículos, nombres propios, etc. que no nos serán de utilidad a la hora de determinar la categoría de una película."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos definir __método que recibirá__ tanto __un conjunto de archivos__ como una __ruta__ donde se encuentran y __contará las palabras__ que aparecen en él __y el número de veces que dichas palabras aparecen__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero antes, definiremos un par de métodos que nos será útiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primero, convertido en método para aportar claridad al código, recibe una palabra y la procesa levemente (la convierte en minúsculas y elimina los carácteres no alfabéticos más típicos) para perder la menor cantidad de información posible (ya que más adelante el algoritmo descartará cualquier palabra que contenga carácteres que no sean alfabéticos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def procesa_palabra(palabra):\n",
    "    palabra = palabra.lower() # Pasamos la palabra a minúscula.\n",
    "    # Para perder la menor información posible, reemplazamos ':', ',', ':' y ';', que son los carácteres más típicos que nos podemos encontrar adyacentes a una palabra y que la invalidarían en el siguiente if del algoritmo.\n",
    "    palabra = palabra.replace('.', '')\n",
    "    palabra = palabra.replace(',', '')\n",
    "    palabra = palabra.replace(':', '')\n",
    "    palabra = palabra.replace(';', '')\n",
    "    \n",
    "    return palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También definimos un método que reciba un solo archivo y cuente sus palabras. Este metodo será usado por el método que estamos buscando y los dividimos de esta forma porque debemos buscar en un solo archivo cuando apliquemos los algoritmos de Naive Bayes y kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cuenta_palabras_desde_archivo(ruta, archivo):\n",
    "    cuenta_palabras = {}\n",
    "    \n",
    "    fichero = open(ruta + \"/\" + archivo, \"r\", encoding=\"latin-1\") # Elegimos latin-1 en vez de utf-8 por problemas con las tildes.\n",
    "\n",
    "    for palabra in fichero.read().split(): # Recorremos el fichero, palabra a palabra.\n",
    "        palabra = procesa_palabra(palabra)\n",
    "        if palabra.isalpha() is True: # Será true cuando todos los caracteres son alfabéticos y hay al menos uno.\n",
    "            if palabra in cuenta_palabras:\n",
    "                cuenta_palabras[palabra] += 1 # Si la palabra ya existe, entonces incrementa en 1 el número de veces que hace aparición.\n",
    "            else:\n",
    "                cuenta_palabras[palabra] = 1 # Si la palabra no existe, la añade (con valor 1 al número de veces que aparece).\n",
    "\n",
    "    cuenta_palabras\n",
    "    return cuenta_palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, definimos el método que estamos buscando en este momento y que se anunciaba antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter # Lo usaremos para añadir un diccionario a otro.\n",
    "\n",
    "def cuenta_palabras_desde_archivos(ruta, archivos):\n",
    "    cuenta_palabras = {}\n",
    "    \n",
    "    for archivo in archivos:\n",
    "        new = cuenta_palabras_desde_archivo(ruta, archivo)\n",
    "        cuenta_palabras = dict(Counter(cuenta_palabras)+Counter(new))\n",
    "    \n",
    "    return cuenta_palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, con propósito de limpiar la salida que obtendremos vamos a establecer un __umbral__ para desechar todas las palabras que se repitan por debajo del mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "umbral_repetición = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y, con el mismo propósito, otro __umbral__ para desechar todas las palabras con una longitud menor a él:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "umbral_longitud = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, por último, un método que use al anterior que, además, nos __ordene las palabras__ (de mayor a menor uso):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cuenta_palabras_desde_archivos_ordenadas(ruta, archivos):\n",
    "    cuenta_palabras = cuenta_palabras_desde_archivos(ruta, archivos)\n",
    "    \n",
    "    lista_ordenada = [] # Usamos una lista para poder ordenar las palabras.\n",
    "    for palabra, contador in cuenta_palabras.items():\n",
    "        lista_ordenada.append((contador, palabra)) # La lista ordenada almacenará una tupla.\n",
    "    \n",
    "    lista_ordenada = sorted(lista_ordenada, reverse = True) # Lo ordenamos y lo invertimos para que las palabras más frecuentes estén arriba.\n",
    "    resultado = lista_ordenada.copy()\n",
    "    \n",
    "    for elemento in lista_ordenada:\n",
    "        if (elemento[0] < umbral_repetición) or (len(elemento[1]) < umbral_longitud): # Si la palabra supera los umbrales indicados, se muestra.\n",
    "            resultado.remove(elemento)\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Palabras más frecuentes por categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Categoría [j] -----\n",
      "[(1, 'tokyo'), (1, 'japon'), (1, 'chino')]\n",
      "----- Categoría [ch] -----\n",
      "[(5, 'chino'), (1, 'shanghai'), (1, 'pekin'), (1, 'macao')]\n"
     ]
    }
   ],
   "source": [
    "for categoría in categorías:\n",
    "    print(\"----- Categoría [%s] -----\" % (categoría))\n",
    "    print(cuenta_palabras_desde_archivos_ordenadas(ruta_conjunto_entrenamiento, archivos_entrenamiento_categoría[categoría]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el estudio, inicializamos manualmente las __palabras clave__ de cada __categoría__, usaremos unas 20 palabras para cada una de ellas (podrán repetirse entre categorías)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palabras_clave_categoría = {}\n",
    "\n",
    "# Las inicializamos como conjuntos ya que no nos interesa el orden y, además, no se permiten duplicados.\n",
    "palabras_clave_categoría[\"acción\"] = {\"policía\", \"coche\", \"agente\", \"auto\", \"escapar\", \"carrera\", \"seguridad\", \"llamada\", \"gasolina\", \"problemas\", \"operación\", \"escapa\", \"venganza\", \"prisión\", \"muerte\", \"hospital\", \"carreras\", \"ataque\", \"arma\", \"asesino\", \"asesina\", \"asesinos\", \"terrorista\", \"terroristas\", \"persecución\", \"muerto\", \"investigación\", \"destrucción\", \"bomba\", \"ruso\"}\n",
    "palabras_clave_categoría[\"comedia\"] = {\"incorrecto\", \"simpáticos\", \"alegre\", \"carcajadas\", \"cómica\", \"cómicos\", \"delirante\", \"divertidos\", \"irreverente\", \"risas\", \"sátira\", \"divertido\", \"divertida\", \"gags\", \"hilarantes\", \"humor\", \"tópicos\", \"crítica\", \"absurdo\", \"hilarante\", \"absurdas\", \"extravagante\"}\n",
    "palabras_clave_categoría[\"terror\"] = {}\n",
    "palabras_clave_categoría[\"bélico\"] = {\"guerra\", \"ejército\", \"sargento\", \"soldados\", \"general\", \"grupo\", \"hombres\", \"combate\", \"misión\", \"unidos\", \"matar\", \"teniente\", \"capitán\", \"alemán\", \"alemanes\", \"vietnam\", \"estadounidense\", \"estadounidenses\", \"soldado\", \"regimiento\", \"operación\", \"oficial\", \"infantería\", \"cabo\", \"enemigo\"}\n",
    "palabras_clave_categoría[\"western\"] = {\"pueblo\", \"hijo\", \"joven\", \"amigo\", \"sheriff\", \"pistolero\", \"banda\", \"guerra\", \"diligencia\", \"camino\", \"duelo\", \"bandidos\", \"granjero\", \"asesino\", \"revólver\", \"arma\", \"armas\", \"desierto\", \"oeste\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es una categoría nueva y __no__ se han añadido palabras clave, vamos a generarlas automáticamente en base a su frecuencia. Para ello, necesitamos el siguiente método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genera_palabras_clave(categoría):\n",
    "    lista_ordenada = cuenta_palabras_desde_archivos_ordenadas(ruta_conjunto_entrenamiento, archivos_entrenamiento_categoría[categoría])\n",
    "    \n",
    "    candidatos = lista_ordenada[:20] # Elegimos los 20 primeros elementos de la lista (no olvidemos que obtenemos una tupla).\n",
    "    \n",
    "    resultado = [elemento[1] for elemento in candidatos] # De la tupla, nos quedamos con el elemento \"1\", que contiene la cadena de texto que repesenta la palabra.\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ejecutar el algoritmo para auto-generar las palabras claves, puesto que el resultado carecerá de revisión, vamos a __endurecer los umbrales__ para así asegurarnos, en la medida de lo posible, que las palabras resultantes serán de mayor calidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "umbral_repetición = 6\n",
    "umbral_longitud = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora es el momento de ejecutar el pequeño algoritmo para auto-generar las palabras clave __si faltan__, para cada categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for categoría in categorías:\n",
    "    if (categoría not in palabras_clave_categoría) or (len(palabras_clave_categoría[categoría]) == 0):\n",
    "        palabras_clave_categoría[categoría] = genera_palabras_clave(categoría)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos las palabras clave de cada categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras clave por categoría:\n",
      "----------\n",
      "Palabras clave de [j]:\n",
      "['tokyo', 'japon', 'chino']\n",
      "----------\n",
      "Palabras clave de [ch]:\n",
      "['chino', 'shanghai', 'pekin', 'macao']\n"
     ]
    }
   ],
   "source": [
    "print(\"Palabras clave por categoría:\")\n",
    "\n",
    "for categoría in categorías:\n",
    "    print(\"----------\")\n",
    "    print(\"Palabras clave de [%s]:\" % (categoría))\n",
    "    print(palabras_clave_categoría[categoría])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, a continuación, añadimos todas las palabras clave de cada categoría a un nuevo conjunto que contenga __todas las palabras clave__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las palabras clave son:\n",
      "{'japon', 'chino', 'macao', 'shanghai', 'tokyo', 'pekin'}\n"
     ]
    }
   ],
   "source": [
    "palabras_clave = set()\n",
    "\n",
    "# Update nos permite añadir el contenido de un set a otro set\n",
    "for categoría in categorías:\n",
    "    palabras_clave.update(palabras_clave_categoría[categoría])\n",
    "\n",
    "print(\"Las palabras clave son:\\n%s\" % (palabras_clave))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, antes de acabar, realizaremos un pequeño estudio relacionado con las palabras clave seleccionadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de palabras clave: \t 6\n",
      "Media de palabras clave por categoría: \t 3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Número total de palabras clave: \\t %s\" % (len(palabras_clave)))\n",
    "print(\"Media de palabras clave por categoría: \\t %s\" % (len(palabras_clave)/len(categorías)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte IV: Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En esta parte se va a llevar a cabo el procesamiento de datos para posteriormente utilizarlos en los algoritmos de __Naive Bayes__ y __kNN__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que se deben realizar cálculos distintos para cada algoritmo, dividiremos esta sección en dos subsecciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte IV-A: Procesamiento de Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar el algoritmo Naive Bayes primero debemos calcular todos los __P(c)__ ___(probabilidad de \"c\")___ y los __P(t|c)__ ___(probabilidad de \"t\" condicionada a \"c\")___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso __\"c\"__ sería nuestra categoría y __\"t\"__ cada palabra clave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, vamos a calcular los __P(c)__. Para ello, tenemos que contar el número de documentos de la categoría en cuestión existentes en nuestro conjunto de entrenamiento y dividirlo entre el número total de documentos de nuestro conjunto de entrenamiento. Así pues, por ejemplo, la probabilidad de acción (__P(acción)__) sería el número resultante de dividir el total de documentos catalogados como \"acción\" de nuestro conjunto de entrenamiento entre el número total de documentos del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tenemos un conjunto con todos los archivos de entrenamiento y un conjunto específico por cada categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(j) = \t 0.250000\n",
      "P(ch) = \t 0.750000\n"
     ]
    }
   ],
   "source": [
    "probabilidad_categoría = {}\n",
    "\n",
    "for categoría in categorías:\n",
    "    probabilidad_categoría[categoría] = len(archivos_entrenamiento_categoría[categoría]) / len(archivos_entrenamiento)\n",
    "    print(\"P(%s) = \\t %f\" % (categoría, probabilidad_categoría[categoría]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo para asegurarnos, todas las probabilidades deben sumar __~1__ en este apartado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de probabilidades (debe ser ~1) \t 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Suma de probabilidades (debe ser ~1) \\t %f\" % (sum([probabilidad_categoría[categoría] for categoría in categorías])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para calcular los __P(t|c)__ será un poco más complejo. Para llevar a cabo esta tarea haremos uso de __un diccionario por cada categoría__ que __relacionará palabras clave con su probabilidad condicionada a la categoría del diccionario__. De nuevo, tomaremos la categoría \"acción\" como ejemplo: el diccionario \"probabilidad_palabraclave_acción\" recogerá, por cada palabra clave, su probabilidad condiccionada a la categoría acción, es decir, su __P(t|acción__), siendo \"t\" cada entrada del diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero antes de empezar, definiremos un método, que utilizaremos en los siguientes pasos, que genere cada diccionario deseado como salida y, además, le __aplique un suavizado de LaPlace__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crea_diccionario_probabilidades_condicionadas(archivos_entrenamiento_categoría, palabras_clave_categoría):\n",
    "    probabilidades_condicionadas = {}\n",
    "    cuenta_palabras_categoría = cuenta_palabras_desde_archivos(ruta_conjunto_entrenamiento, archivos_entrenamiento_categoría) # Almacena las palabras clave de la categoría y el número de veces que se repiten.\n",
    "    \n",
    "    número_palabras_clave_totales = len(palabras_clave) # Número de palabras clave que poseemos en total.\n",
    "    número_palabras_clave_categoría = sum(cuenta_palabras_categoría.values())\n",
    "    \n",
    "    for palabra_clave in palabras_clave:\n",
    "        if palabra_clave in cuenta_palabras_categoría:\n",
    "            número_veces_aparece_palabra_en_categoría = cuenta_palabras_categoría[palabra_clave] # Número de veces que la palabra clave se repite en esta categoría.\n",
    "        else:\n",
    "            número_veces_aparece_palabra_en_categoría = 0\n",
    "        result = ((número_veces_aparece_palabra_en_categoría + 1) / (número_palabras_clave_categoría + número_palabras_clave_totales)) # Añadimos 1 en el numerador y el número de palabras claves totales en el denominador para aplicar el suavizado.\n",
    "        probabilidades_condicionadas[palabra_clave] = result\n",
    "    \n",
    "    return probabilidades_condicionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También vamos a crear un pequeño método que nos ayude a visualizar las probabilidades condicionadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mostrar_diccionario_probabilidades_condicionadas(diccionario, categoría):\n",
    "    for entrada in diccionario:\n",
    "        print(\"P(%s|%s) = \\t %f\" % (entrada, categoría, diccionario[entrada]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, empezamos con el cálculo en sí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- j -----\n",
      "P(japon|j) = \t 0.222222\n",
      "P(chino|j) = \t 0.222222\n",
      "P(macao|j) = \t 0.111111\n",
      "P(shanghai|j) = \t 0.111111\n",
      "P(tokyo|j) = \t 0.222222\n",
      "P(pekin|j) = \t 0.111111\n",
      "----- ch -----\n",
      "P(japon|ch) = \t 0.071429\n",
      "P(chino|ch) = \t 0.428571\n",
      "P(macao|ch) = \t 0.142857\n",
      "P(shanghai|ch) = \t 0.142857\n",
      "P(tokyo|ch) = \t 0.071429\n",
      "P(pekin|ch) = \t 0.142857\n"
     ]
    }
   ],
   "source": [
    "probabilidad_palabraclave = {}\n",
    "\n",
    "for categoría in categorías:\n",
    "    probabilidad_palabraclave[categoría] = crea_diccionario_probabilidades_condicionadas(archivos_entrenamiento_categoría[categoría], palabras_clave_categoría[categoría])\n",
    "    \n",
    "    print(\"----- %s -----\" % (categoría))\n",
    "    mostrar_diccionario_probabilidades_condicionadas(probabilidad_palabraclave[categoría], categoría)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculadas todas las probabilidades condicionadas de todas las categorías ya hemos finalizado con este subapartado, pero antes vamos a recordar un par de cosas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acceder a una probabilidad específica símplemente ejecutamos lo siguiente (para el ejemplo obtendremos la __probabilidad de coche__ condicionada a la categoría __acción__, es decir, __P(coche|acción)__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"P(coche|acción) = %f\" % (probabilidad_palabraclave[\"acción\"][\"coche\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hubiese una __palabra clave que no apareciese en acción, la probabilidad no sería 0__ ya que estamos usando suavizado. Por ejemplo, podemos comprobarlo con la probabilidad de soldado (que no forma parte de las palabras clave de acción) condicionada a acción (__P(soldado|acción)__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"P(gags|acción) = %f\" % (probabilidad_palabraclave[\"acción\"][\"gags\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte IV-B: Procesamiento de kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chino': 6, 'shanghai': 1, 'macao': 1, 'tokyo': 1, 'japon': 1, 'pekin': 1}\n"
     ]
    }
   ],
   "source": [
    "palabras_clave_y_repeticiones = {}\n",
    "\n",
    "cuenta_palabras = cuenta_palabras_desde_archivos(ruta_conjunto_entrenamiento, archivos_entrenamiento)\n",
    "\n",
    "for palabra in cuenta_palabras:\n",
    "    if palabra in palabras_clave:\n",
    "        palabras_clave_y_repeticiones[palabra] = cuenta_palabras[palabra]\n",
    "\n",
    "print(palabras_clave_y_repeticiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculo_peso(palabra_clave, ruta, archivo):\n",
    "    frecuencia_en_documento = 0\n",
    "    frecuencia_documental = 0\n",
    "    frecuencia_documental_inversa = 0\n",
    "    peso = 0\n",
    "    \n",
    "    cuenta_palabras = cuenta_palabras_desde_archivo(ruta, archivo)\n",
    "    if palabra_clave in cuenta_palabras:\n",
    "        frecuencia_en_documento = cuenta_palabras[palabra_clave]\n",
    "    \n",
    "    frecuencia_documental = palabras_clave_y_repeticiones[palabra_clave]\n",
    "    \n",
    "    if frecuencia_documental is not 0:\n",
    "        aux = len(archivos_entrenamiento) / frecuencia_documental\n",
    "    else:\n",
    "        aux = 0\n",
    "    \n",
    "    frecuencia_documental_inversa = math.log(aux)\n",
    "    \n",
    "    peso = frecuencia_en_documento * frecuencia_documental_inversa\n",
    "    \n",
    "    return peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculo_pesos(ruta, archivo):\n",
    "    lista_pesos = []\n",
    "    \n",
    "    for palabra_clave in palabras_clave:\n",
    "        lista_pesos.append(calculo_peso(palabra_clave, ruta, archivo))\n",
    "    \n",
    "    return lista_pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "diccionario_palabraclave_pesos = {}\n",
    "\n",
    "for archivo in archivos_entrenamiento:\n",
    "    lista_pesos_archivo = calculo_pesos(ruta_conjunto_entrenamiento, archivo)\n",
    "    \n",
    "    diccionario_palabraclave_pesos[archivo] = lista_pesos_archivo\n",
    "\n",
    "# print(diccionario_palabraclave_pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte V: Salvado del procesamiento en fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte V-A: Salvado del procesamiento de Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el enunciado de la práctica requiere que guardemos el procesado que acabamos que realizar (en la parte IV) para después utilizarlo en la ejecución de los algoritmos, procedemos a ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero necesitamos crear dos grandes listas: la primera contendrá la cadena relacionada con la probabilidad de que pase algo y la segunda la probabilidad de que pase. Ambas listas deben ser iguales y debemos almacenar toda la información obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lista_texto = []\n",
    "lista_valor = []\n",
    "\n",
    "for categoría in categorías:\n",
    "    lista_texto.append(\"P(\" + categoría + \")\")\n",
    "    lista_valor.append(probabilidad_categoría[categoría])\n",
    "    \n",
    "    for entrada in probabilidad_palabraclave[categoría]:\n",
    "        lista_texto.append(\"P(\" + entrada + \"|\" + categoría + \")\")\n",
    "        lista_valor.append(probabilidad_palabraclave[categoría][entrada])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv # Librería que necesitaremos para guardar en formato \".csv\".\n",
    "\n",
    "csvfile = \"csv/naive-bayes.csv\"\n",
    "datos_a_guardar = zip(lista_texto, lista_valor)\n",
    "\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for dato in datos_a_guardar:\n",
    "        writer.writerow([dato[0]] + [dato[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte V-B: Salvado del procesamiento de kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvfile = \"csv/knn.csv\"\n",
    "datos_a_guardar = diccionario_palabraclave_pesos\n",
    "\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for dato in datos_a_guardar:\n",
    "        writer.writerow([dato] + [datos_a_guardar[dato]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte VI: Ejecución de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la parte II ya encontramos los ficheros del conjunto de test, que están en su respectiva carpeta, ahora tenemos que procesaros igual que hicimos en la parte III con los ficheros del conjunto de pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir un método que nos será de utilidad: a partir de un __csv__ dado y de una __cadena a buscar__, devolverá el valor relacionado con dicha celda del csv. Esto nos será útil para, a partir de la cadena de una probabilidad (por ejemplo, _P(bala|acción)_), nos devuelva su probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lee_fichero(nombre_csv):\n",
    "    with open(nombre_csv, 'rt', encoding=\"latin-1\") as fichero:\n",
    "        lector = csv.reader(fichero)\n",
    "        diccionario = dict(lector)\n",
    "    \n",
    "    return diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte VI-A: Ejecución de Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método naive_bayes recibe un archivo y los datos procesados (probabilidades) como parámetros y determina la categoría del archivo pasado como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_bayes(archivo, csv):\n",
    "    cuenta_palabras = cuenta_palabras_desde_archivo(ruta_conjunto_prueba, archivo)\n",
    "    \n",
    "    palabras_coincidentes_con_palabras_clave = cuenta_palabras.copy()\n",
    "    \n",
    "    # De las palabras que contiene el fichero, desechamos todas las que no coinciden con las palabras clave.\n",
    "    for palabra in cuenta_palabras:\n",
    "        if palabra not in palabras_clave:\n",
    "            del palabras_coincidentes_con_palabras_clave[palabra]\n",
    "    \n",
    "    # Abrimos el fichero \".csv\" generado para consultar datos en el siguiente paso.\n",
    "    datos = lee_fichero(csv)\n",
    "    \n",
    "    # Ejecutamos el algoritmo en sí.\n",
    "    resultados = {} # Resultados será un diccionario que contendrá la categoría y la \"puntuación\" otorgada por el algoritmo a esa categoría (para posteriormente elegir la categoría con el máximo valor).\n",
    "    \n",
    "    for categoría in categorías:\n",
    "        probabilidades_condicionadas_a_multiplicar = []\n",
    "        for palabra_clave in palabras_coincidentes_con_palabras_clave:\n",
    "            cadena_a_buscar = \"P(\" + palabra_clave + \"|\" + categoría + \")\" # Define la cadena que se debe buscar en el archivo. En este caso la probabilidad condicionada a la categoría.\n",
    "            probabilidades_condicionadas_a_multiplicar.append(float(datos[cadena_a_buscar]) ** palabras_coincidentes_con_palabras_clave[palabra_clave]) # Busca el valor de la probabilidad condicionada requerida (lo transforma en float), lo eleva al número de veces que se repite y añade el valor de la probabilidad condicionada hallada a una lista que se pasará a multiplicar después.\n",
    "        \n",
    "        cadena_a_buscar = \"P(\" + categoría + \")\" # Define la cadena que se debe buscar en el archivo. En este caso la probabilidad de la categoría.\n",
    "        probabilidad_categoría = float(datos[cadena_a_buscar])\n",
    "        \n",
    "        # Multiplicamos los elementos de la lista de probabilidades condicionadas\n",
    "        probabilidades_condicionadas_multiplicadas = 1.0\n",
    "        for elemento in probabilidades_condicionadas_a_multiplicar:\n",
    "            probabilidades_condicionadas_multiplicadas = probabilidades_condicionadas_multiplicadas * elemento\n",
    "        \n",
    "        resultados[categoría] = probabilidades_condicionadas_multiplicadas * probabilidad_categoría # Calcula el coeficiente\n",
    "    \n",
    "    result = max(resultados, key=resultados.get) # Devuleve el resultado del algoritmo. En este caso el elemento del diccionario con mayor coeficiente.\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ejecutamos el algoritmo__ e __imprimimos__ los __resultados__ obtenidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aplicar_naive_bayes_archivos_prueba():\n",
    "    print(\"El resultado de aplicar el algoritmo [Naive-Bayes] al conjunto de pruebas es...\")\n",
    "    for archivo in archivos_prueba:\n",
    "        algoritmo = naive_bayes(archivo, \"csv/naive-bayes.csv\")\n",
    "        print(\"[%s] \\t [%s]\" % (archivo.replace(\".txt\", \"\"), algoritmo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El resultado de aplicar el algoritmo [Naive-Bayes] al conjunto de pruebas es...\n",
      "[A todo gas - Tokyo race] \t [ch]\n"
     ]
    }
   ],
   "source": [
    "aplicar_naive_bayes_archivos_prueba()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte VI-B: Ejecución de kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcula_distancia(v, w):\n",
    "    # Comprueba que las listas son del mismo tamaño.\n",
    "    if(len(v) == len(w)):\n",
    "        numerador = sum([elemento_v * elemento_w for elemento_v, elemento_w in zip(v,w)])\n",
    "        \n",
    "        denominador_parte_v = math.sqrt(sum([elemento_v ** 2 for elemento_v in v]))\n",
    "        denominador_parte_w = math.sqrt(sum([elemento_w ** 2 for elemento_w in w]))\n",
    "        \n",
    "        denominador = denominador_parte_v * denominador_parte_w\n",
    "        \n",
    "        return numerador / denominador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn(archivo, csv):\n",
    "    v = calculo_pesos(ruta_conjunto_prueba, archivo)\n",
    "    \n",
    "    # Abrimos el fichero \".csv\" generado para consultar datos en el siguiente paso.\n",
    "    datos = lee_fichero(csv)\n",
    "    \n",
    "    # Ejecutamos el algoritmo en sí.\n",
    "    resultados = {} # Resultados será un diccionario que contendrá el archivo y la \"puntuación\" (similitud) otorgada por el algoritmo a esa categoría (para posteriormente elegir la categoría del archivo con la similitud más cercana a uno, que será el mayor valor).\n",
    "    \n",
    "    # Ahora que tenemos el peso del archivo a clasificar mediante el algoritmo y los pesos de los archivos del conjunto de entrenamiento (extraídos del \".csv\" y guardados en forma de diccionario) tenemos que calcular, una por una, la distancia a cada elemento del conjunto de entrenamiento y quedarnos con la menor.\n",
    "    for dato in datos:\n",
    "        # Como lo que guardamos es una cadena, es necesario un pequeño procesamiento para transformarlo de nuevo en una lista.\n",
    "        w = datos[dato].replace('[', '') # Primero eliminamos \"[\".\n",
    "        w = w.replace(']', '') # Hacemos lo mismo con \"]\".\n",
    "        w = w.split(\",\") # Aplicamos \".split()\" para volver a \"trocear\" la cadena y convertirla de nuevo en una lista.\n",
    "        w = [float(elemento) for elemento in w]\n",
    "        resultados[dato] = calcula_distancia(v, w) # \"v\" son los pesos del archivo a clasificar y \"w\" los del archivo del conjunto de entrenamiento que está siendo procesado.\n",
    "    \n",
    "    # Para una mayor exactitud, saber en qué categoría se enmarcará la muestra y a qué película se debe, usaremos una lista para devolver dicha información (el elemento 0 contendrá la categoría y el 1 la película de dónde procede).\n",
    "    result = max(resultados, key=resultados.get)\n",
    "    result = result.split(\"/\")\n",
    "    \n",
    "    return result[0], result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aplicar_knn_archivos_prueba():\n",
    "    print(\"El resultado de aplicar el algoritmo [kNN] al conjunto de pruebas es...\")\n",
    "    for archivo in archivos_prueba:\n",
    "        algoritmo = knn(archivo, \"csv/knn.csv\")\n",
    "        print(\"[%s] \\t [%s] (por similitud con [%s])\" % (archivo.replace(\".txt\", \"\"), algoritmo[0], algoritmo[1].replace(\".txt\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El resultado de aplicar el algoritmo [kNN] al conjunto de pruebas es...\n",
      "[A todo gas - Tokyo race] \t [j] (por similitud con [Apocalipsis now])\n"
     ]
    }
   ],
   "source": [
    "aplicar_knn_archivos_prueba()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte VII: Análisis de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte VIII: Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
